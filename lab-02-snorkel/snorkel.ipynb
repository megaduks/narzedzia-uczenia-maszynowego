{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snorkel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this lab is to introduce students to the [Snorkel](http://www.snorkel.org) tool and the possibilities of programmatic label generation using the weak-supervised learning paradigm.\n",
    "\n",
    "In order to use weakly supervised learning to generate labels, it is necessary to create three datasets:\n",
    "\n",
    "- **train set**: which does not have any labels\n",
    "- **validation set**: used for hyperparameter optimization, has labels\n",
    "- **test set**: used only for final model evaluation, has labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling functions\n",
    "\n",
    "The first step will be to load the dataset and split it into a train set and a test set. Since in our set all SMS have a label, we will simulate a weakly supervised learning problem by randomly removing 80% of the labels. Additionally, `snorkel` requires numeric labels, so we need to recode the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T11:40:49.401132Z",
     "start_time": "2021-05-20T11:40:49.281794Z"
    }
   },
   "outputs": [],
   "source": [
    "!head data/smsspamcollection.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T11:45:24.263858Z",
     "start_time": "2021-05-20T11:45:23.823215Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('max_colwidth', 600)\n",
    "\n",
    "SPAM = 1\n",
    "HAM = 0\n",
    "ABSTAIN = -1\n",
    "\n",
    "df = pd.read_csv('./data/smsspamcollection.csv', \n",
    "                 sep='\\t', \n",
    "                 header=None, \n",
    "                 names=['old_label', 'text'])\n",
    "\n",
    "df['label'] = df.old_label.apply(lambda x: SPAM if x == 'spam' else HAM)\n",
    "\n",
    "df.loc[df.sample(frac=0.8).index, 'label'] = ABSTAIN\n",
    "df.drop(columns=['old_label'], inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T11:45:58.164302Z",
     "start_time": "2021-05-20T11:45:58.155166Z"
    }
   },
   "outputs": [],
   "source": [
    "abstain_idx = df.label == ABSTAIN\n",
    "\n",
    "abstain_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T11:46:53.228962Z",
     "start_time": "2021-05-20T11:46:53.223513Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = df[abstain_idx]\n",
    "df_test = df[~abstain_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T11:47:03.923525Z",
     "start_time": "2021-05-20T11:47:03.918442Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Simple keyword search\n",
    "\n",
    "As a first example, we will use a search for the words \"check\" and \"free\" in SMS content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T11:50:56.360803Z",
     "start_time": "2021-05-20T11:50:55.564036Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "\n",
    "@labeling_function()\n",
    "def check(sms):\n",
    "    return SPAM if \"check\" in sms.text.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def free(sms):\n",
    "    return SPAM if \"free\" in sms.text.lower() else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The next step is to apply the labeling functions to the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T11:51:50.137340Z",
     "start_time": "2021-05-20T11:51:49.719664Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from snorkel.labeling import PandasLFApplier\n",
    "\n",
    "lfs = [check, free]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The result of applying the set of labeling functions to the train set is a matrix of size $m \\times n$, where $m$ is the number of examples and $n$ is the number of labeling functions. The matrix contains the result of applying each function to each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T11:53:40.043517Z",
     "start_time": "2021-05-20T11:53:40.036328Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "L_train[200:250,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T11:52:58.322757Z",
     "start_time": "2021-05-20T11:52:58.316818Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "L_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T11:53:56.202942Z",
     "start_time": "2021-05-20T11:53:56.193899Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_train.iloc[200,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The simplest way to analyze this is to determine the coverage of labeling functions (i.e., the percentage of cases for which the function returned a result other than `ABSTAIN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T11:54:39.205708Z",
     "start_time": "2021-05-20T11:54:39.198933Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "coverage_check, coverage_free = (L_train != ABSTAIN).mean(axis=0)\n",
    "\n",
    "print(f\"Coverage for check(): {coverage_check * 100:.1f}%\")\n",
    "print(f\"Coverage for free(): {coverage_free * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Fortunately, `snorkel` offers additional tools that allow for deeper analysis of the result of labeling functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T11:55:24.778805Z",
     "start_time": "2021-05-20T11:55:24.733963Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The meaning of each column is as follows:\n",
    "- `Polarity`: the set of labels returned by the function\n",
    "- `Coverage`: the percentage of examples for which the function returns a value other than `ABSTAIN`\n",
    "- Overlaps: the percentage of examples for which at least one other labeling function returned a value\n",
    "- Conflicts: the percentage of examples for which at least one other labeling function returned a different value\n",
    "\n",
    "If the train set contained labels, the method would also return:\n",
    "- `Correct`: the number of correct labels\n",
    "- `Incorrect`: number of incorrect labels\n",
    "- `Empirical Accuracy`: the percentage of correct labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's check the examples labeled by the `free()` function as spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T11:57:31.191746Z",
     "start_time": "2021-05-20T11:57:31.174732Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_train.iloc[L_train[:,1] == SPAM].sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It seems that the phrase \"call now\" is also a good indicator for spam. So let's add one more labeling function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T11:59:45.533665Z",
     "start_time": "2021-05-20T11:59:45.229141Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def call_now(sms):\n",
    "    return SPAM if \"call now\" in sms.text.lower() else ABSTAIN\n",
    "\n",
    "lfs = [check, free, call_now]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T11:59:49.435271Z",
     "start_time": "2021-05-20T11:59:49.413422Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's see which examples were labeled as spam by the `call_now()` function but omitted by `free()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:01:58.735839Z",
     "start_time": "2021-05-20T12:01:58.716761Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from snorkel.analysis import get_label_buckets\n",
    "\n",
    "buckets = get_label_buckets(L_train[:, 1], L_train[:, 2])\n",
    "buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:02:17.973420Z",
     "start_time": "2021-05-20T12:02:17.968425Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "buckets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:04:19.982798Z",
     "start_time": "2021-05-20T12:04:19.969924Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_train.iloc[buckets[(SPAM, ABSTAIN)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:04:35.711741Z",
     "start_time": "2021-05-20T12:04:35.684211Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### assignment\n",
    "\n",
    "Write a labeling function that marks as spam all messages containing the word \"HOT\" written in capitals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:05:36.255287Z",
     "start_time": "2021-05-20T12:05:36.250869Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def hot(sms):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Searching based on a regular expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Another type of labeling function is one that uses regexp to find specific expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:12:54.916124Z",
     "start_time": "2021-05-20T12:12:54.372271Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "@labeling_function()\n",
    "def regex_I_am_free(sms):\n",
    "    if re.search(r\"\\sI\\s.*free\", sms.text, flags=re.I):\n",
    "        return HAM\n",
    "    elif re.search(r\"free\", sms.text, flags=re.I):\n",
    "        return SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "lfs = [check, free, call_now, regex_I_am_free]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's compare examples that the `free()` function labels as spam and the `regex_I_am_free()` function considers valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:12:58.280185Z",
     "start_time": "2021-05-20T12:12:58.261276Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "buckets = get_label_buckets(L_train[:, 1], L_train[:, 3])\n",
    "df_train.iloc[buckets[(SPAM, HAM)]].sample(10, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### assignment\n",
    "\n",
    "Write a labeling function that will mark as spam all messages containing any amounts specified with a currency symbol ($99, £1.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:15:46.493051Z",
     "start_time": "2021-05-20T12:15:46.489530Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def contains_money(sms):\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Searching based on heuristics\n",
    "\n",
    "A simple heuristic to find spam is to assume that if more than 10% of the message text is written in capitals, there is a good chance it is spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:17:15.636442Z",
     "start_time": "2021-05-20T12:17:14.969429Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def has_many_uppercase_words(sms):\n",
    "    percentage_uppercase = sum([word.isupper() for word in sms.text.split()]) / len(sms.text.split())\n",
    "    \n",
    "    return SPAM if percentage_uppercase > 0.1 else ABSTAIN\n",
    "\n",
    "lfs = [check, free, call_now, regex_I_am_free, has_many_uppercase_words]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### assignment\n",
    "\n",
    "Write a labeling function that marks as valid those messages that are shorter than 10 words and do not contain any word written in capitals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:21:26.787973Z",
     "start_time": "2021-05-20T12:21:26.782348Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def short_and_no_uppercase(sms):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Using an external statistical model\n",
    "\n",
    "When labeling data, you can use external models whose response can be important information for deciding how to label an example. `snorkel` has several built-in integrations in the form of the `Preprocessor` interface, in the example below we will use the `SpaCy` library to perform additional grammatical analysis of the text. However, you will need to download the English language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:22:53.769897Z",
     "start_time": "2021-05-20T12:22:51.842107Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:23:55.309153Z",
     "start_time": "2021-05-20T12:23:55.233577Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_text = \"\"\"England is a country that is part of the United Kingdom. \n",
    "It shares land borders with Wales to its west and Scotland to its north. \n",
    "The Irish Sea lies northwest of England and the Celtic Sea to the southwest. \n",
    "England is separated from continental Europe by the North Sea to the east and the \n",
    "English Channel to the south. The country covers five-eighths of the island of \n",
    "Great Britain, which lies in the North Atlantic, and includes over 100 smaller islands, \n",
    "such as the Isles of Scilly and the Isle of Wight.\"\"\"\n",
    "\n",
    "doc = nlp(_text)\n",
    "\n",
    "for e in doc.ents:\n",
    "    print(f\"{e.text:<25} start:{e.start} end:{e.end} label: {e.label_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:26:46.216839Z",
     "start_time": "2021-05-20T12:26:45.068825Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "\n",
    "spacy_preprocessor = SpacyPreprocessor(text_field=\"text\", doc_field=\"doc\", memoize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Assume that short text messages in which a reference to a specific person appears are not spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:26:51.718756Z",
     "start_time": "2021-05-20T12:26:51.712363Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:29:03.396477Z",
     "start_time": "2021-05-20T12:29:03.392419Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@labeling_function(pre=[spacy_preprocessor])\n",
    "def has_person(sms):\n",
    "    if len(sms.doc) < 20 and any([ent.label_ == \"PERSON\" for ent in sms.doc.ents]):\n",
    "        return HAM\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:32:12.737018Z",
     "start_time": "2021-05-20T12:29:10.762089Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lfs = [check, free, call_now, regex_I_am_free, has_many_uppercase_words, has_person]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Another example of pre-processing data for labeling would be determining the average word frequency of a document. Below we define a function that determines the average word frequency and we decorate it as an example of a pre-processor. When a text message is sent to the next labeling function, the pre-processor will populate the text message with the average word frequency and, based on that, the labeling function will make a decision (we assume that if the text message contains many rare words then it is spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:34:50.400280Z",
     "start_time": "2021-05-20T12:34:50.242212Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from wordfreq import zipf_frequency\n",
    "from snorkel.preprocess import preprocessor\n",
    "\n",
    "@preprocessor(memoize=True)\n",
    "def avg_word_freq(sms):\n",
    "    sms.avg_word_freq = sum([zipf_frequency(word, 'en') for word in sms.text.split()]) / len(sms.text.split())\n",
    "    \n",
    "    return sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:35:33.288973Z",
     "start_time": "2021-05-20T12:35:33.285681Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@labeling_function(pre=[avg_word_freq])\n",
    "def many_rare_words(sms):\n",
    "    return ABSTAIN if sms.avg_word_freq >= 4 else SPAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:35:44.679808Z",
     "start_time": "2021-05-20T12:35:38.912041Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lfs = [check, free, call_now, regex_I_am_free, has_many_uppercase_words, has_person, many_rare_words]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:36:12.194263Z",
     "start_time": "2021-05-20T12:36:12.172027Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train.iloc[L_train[:,6] == SPAM].sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### assignment\n",
    "\n",
    "Write a labeling function that marks messages containing more than 3 adjectives as spam. Use the SpaCy library for pre-processing. \n",
    "\n",
    "__Hint__: the following example shows how to read the part-of-speech label for each token from the message being analyzed. For information on all token properties recognized by SpaCy, see [API documentation](https://spacy.io/api/token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:37:42.605063Z",
     "start_time": "2021-05-20T12:37:41.616025Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sms = \"Yetunde, i'm sorry but moji and i seem too busy to be able to go shopping.\"\n",
    "\n",
    "for token in nlp(sms):\n",
    "    print(f\"{token.text:<10} {token.pos_:<10} {token.tag_:<10} {token.lemma_:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Combining labeling functions into a single model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The goal of labeling functions is not to achieve individually large coverage. Labeling functions are inherently noisy and can make many individual errors. The true utility of labeling functions becomes apparent when multiple functions are combined to form a single model.\n",
    "\n",
    "We will first build a simple model based on majority voting, and then build a more complex model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:40:41.883129Z",
     "start_time": "2021-05-20T12:39:50.619588Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lfs = [check, free, call_now, regex_I_am_free, has_person, many_rare_words]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "L_test = applier.apply(df=df_test)\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:40:52.322974Z",
     "start_time": "2021-05-20T12:40:52.300671Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "LFAnalysis(L=L_test, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:41:56.215965Z",
     "start_time": "2021-05-20T12:41:55.005575Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from snorkel.labeling.model import MajorityLabelVoter\n",
    "\n",
    "majority_model = MajorityLabelVoter()\n",
    "preds_train = majority_model.predict(L=L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:41:58.087921Z",
     "start_time": "2021-05-20T12:41:58.079332Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:42:13.763912Z",
     "start_time": "2021-05-20T12:42:13.758310Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "labels, counts = np.unique(preds_train, return_counts=True)\n",
    "\n",
    "for l, c in zip(labels, counts):\n",
    "    print(f\"LABEL: {l}, count: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:43:41.439457Z",
     "start_time": "2021-05-20T12:43:40.719964Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train=L_train, n_epochs=500, log_freq=100, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:44:48.947300Z",
     "start_time": "2021-05-20T12:44:48.882307Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "majority_acc = majority_model.score(L=L_test, Y=df_test.label, tie_break_policy=\"random\")[\"accuracy\"]\n",
    "print(f\"{'Majority voting accuracy:':<25} {majority_acc * 100:.1f}%\")\n",
    "\n",
    "label_model_acc = label_model.score(L=L_test, Y=df_test.label, tie_break_policy=\"random\")[\"accuracy\"]\n",
    "print(f\"{'Probabilistic model accuracy:':<25} {label_model_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Unfortunately, some data points will not receive any label. It is necessary to filter out these points before sending the labeling result for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:51:14.291797Z",
     "start_time": "2021-05-20T12:51:14.217622Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "from snorkel.utils import preds_to_probs, probs_to_preds\n",
    "\n",
    "preds_train, probs_train = label_model.predict(L=L_train, return_probs=True)\n",
    "\n",
    "df_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(X=df_train, y=probs_train, L=L_train)\n",
    "df_train.shape, df_train_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As you can see, we were able to quickly prepare labels for about 650 examples (recall that initially no example in the `df_train` set had labels).\n",
    "\n",
    "The next step will use prepared labels as training data for the actual classifier. We will use simple [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), first pre-processing the input data. Since we are working with text, we will use the [word vector representation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) created based on 5-grams by `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:51:37.552906Z",
     "start_time": "2021-05-20T12:51:37.427111Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from snorkel.utils import probs_to_preds\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "preds_train_filtered = probs_to_preds(probs=probs_train_filtered)\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "X_train = vectorizer.fit_transform(df_train_filtered.text.tolist())\n",
    "X_test = vectorizer.transform(df_test.text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:51:38.273204Z",
     "start_time": "2021-05-20T12:51:38.267576Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:51:39.395481Z",
     "start_time": "2021-05-20T12:51:39.310543Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sklearn_model = LogisticRegression(C=1e3, solver='lbfgs')\n",
    "sklearn_model.fit(X=X_train, y=preds_train_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:51:40.115410Z",
     "start_time": "2021-05-20T12:51:40.108086Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Logistic regression accuracy: {sklearn_model.score(X=X_test, y=df_test.label) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As can be seen, the final model improved the score over the majority vote and the `LabelModel` model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### assignment\n",
    "\n",
    "Complete the above calls with functions that you wrote yourself and check whether your functions improve the quality of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing functions\n",
    "\n",
    "The third functionality offered by the `snorkel` library is the ability to programmatically define data slices to which independent quality assessment measures of the classifier can then be applied. This allows us to have much more precise control over the quality of the model's performance, for example, enabling the training of a model that achieves very good results for a specific subset of data (e.g. precise recognition of a pedestrian silhouette in a picture taken by an autonomous car) at the cost of quality for other types of objects.\n",
    "\n",
    "We will start by defining a slice that contains text messages in which the contents contain the names of popular phone brands.\n",
    "\n",
    "Each slicing function returns a binary value indicating whether a given point belongs to the slice or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('max_colwidth', 400)\n",
    "\n",
    "SPAM = 1\n",
    "HAM = 0\n",
    "ABSTAIN = -1\n",
    "\n",
    "df = pd.read_csv('./data/smsspamcollection.csv', sep='\\t', header=None, names=['old_label', 'text'])\n",
    "\n",
    "df['label'] = df.old_label.apply(lambda x: SPAM if x == 'spam' else HAM)\n",
    "df.drop(columns=['old_label'], inplace=True)\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining and visualizing slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.slicing import slicing_function\n",
    "\n",
    "@slicing_function()\n",
    "def contains_phone_name(sms):\n",
    "    phone_names = ['iphone', 'samsung', 'nokia', 'xiaomi', 'galaxy']\n",
    "    \n",
    "    return any([phone in sms.text.lower() for phone in phone_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see which messages belong to the slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.slicing import slice_dataframe\n",
    "\n",
    "sfs = [contains_phone_name]\n",
    "\n",
    "contains_phone_name_df = slice_dataframe(df_train, contains_phone_name)\n",
    "\n",
    "contains_phone_name_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assignment\n",
    "1. Write a slicing function that will extract those text messages in which at least 20% of the text is written in capital letters.\n",
    "\n",
    "2. Write a slicing function that will extract those text messages in which a sequence of at least three exclamation marks occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@slicing_function()\n",
    "def at_least_20_pct_uppercase(x):\n",
    "    ...\n",
    "\n",
    "@slicing_function()\n",
    "def at_least_3_exclamations(x):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating metrics within a slice\n",
    "\n",
    "In the next example, we will see how to use a slicing function to calculate the measure of model quality only within a specific subset. We will use a simple logistic regression model, first converting SMS messages into a vector form. In this example, we select only the 300 most common words for vectorization, as using the full set causes the neural network we will build at the end of the exercise to take a long time to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), max_features=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_train = [r.text for i, r in df_train.iterrows()]\n",
    "messages_test = [r.text for i, r in df_test.iterrows()]\n",
    "\n",
    "features_train = vectorizer.fit_transform(messages_train)\n",
    "features_test = vectorizer.transform(messages_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = features_train.toarray()\n",
    "y_train = df_train.label.values\n",
    "\n",
    "X_test = features_test.toarray()\n",
    "y_test = df_test.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sklearn_model = LogisticRegression(C=0.01, solver=\"liblinear\")\n",
    "sklearn_model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.utils import preds_to_probs\n",
    "\n",
    "preds_test = sklearn_model.predict(X_test)\n",
    "probs_test = preds_to_probs(preds_test, 2)\n",
    "\n",
    "preds_test[:5], probs_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(f\"F1 score on test set: {100 * f1_score(y_test, preds_test)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trained classifier, we can now apply the slicing function to create a slice and compare the selected metric on the entire dataset and on the defined slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.slicing import PandasSFApplier\n",
    "from snorkel.analysis import Scorer\n",
    "\n",
    "applier = PandasSFApplier(sfs)\n",
    "slice_test = applier.apply(df_test)\n",
    "\n",
    "scorer = Scorer(metrics=[\"f1\"])\n",
    "\n",
    "scorer.score_slices(\n",
    "    S=slice_test, \n",
    "    golds=y_test, \n",
    "    preds=preds_test, \n",
    "    probs=probs_test, \n",
    "    as_dataframe=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assignment\n",
    "\n",
    "Prepare a simple predictive model using [random forest algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) or [simple decision tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) and compare the accuracy of the model on the entire dataset and on slices defined by the functions `contains_phone_name`, `at_least_20_pct_uppercase`, and `at_least_3_exclamations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "sfs = [\n",
    "    contains_phone_name,\n",
    "    at_least_20_pct_uppercase,\n",
    "    at_least_3_exclamations\n",
    "]\n",
    "\n",
    "\n",
    "sklearn_model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using multiple slicing functions\n",
    "\n",
    "In the next step, we will prepare a larger number of slicing functions that will define different planes of analysis of the dataset for us. We will define a function that takes any keywords as arguments and a function that operates on a regular expression. Notice that the first example does not use a decorator to define the slicing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from snorkel.slicing import SlicingFunction\n",
    "\n",
    "def slice_by_keywords(keywords: List[str]):\n",
    "    \"\"\"Returns whether text contains any of the provided keywords\"\"\"\n",
    "    def find_keyword(x: str, keywords: List[str]) -> bool:\n",
    "        return any(word in x.text.lower() for word in keywords)\n",
    "    \n",
    "    return SlicingFunction(\n",
    "        name=f\"keyword_{keywords[0]}\",\n",
    "        f=find_keyword,\n",
    "        resources=dict(keywords=keywords),\n",
    "    )\n",
    "\n",
    "\n",
    "keyword_free = slice_by_keywords(keywords=[\"free\", \"freeee\"])\n",
    "keyword_guarantee = slice_by_keywords(keywords=[\"guarantee\", \"guaranteed\"])\n",
    "\n",
    "@slicing_function()\n",
    "def contains_dotcom_link(x: str):\n",
    "    \"\"\"Returns whether text matches common pattern for \".com\" links.\"\"\"\n",
    "    return bool(re.search(r\"\\w+\\.com\", x.text))\n",
    "\n",
    "@slicing_function()\n",
    "def contains_monetary_value(x: str):\n",
    "    \"\"\"Returns whether text contains any monetary value in the form of £xxx, £ xxx, xxx £, or xxx£\"\"\"\n",
    "    return bool(re.search(r\"£\\s?\\d+|\\d+\\s?£\", x.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the F1 score for the classifier independently for each slice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs = [contains_phone_name, contains_monetary_value, contains_dotcom_link, keyword_free, keyword_guarantee]\n",
    "slice_names = [sf.name for sf in sfs]\n",
    "\n",
    "applier = PandasSFApplier(sfs)\n",
    "slice_test = applier.apply(df_test)\n",
    "\n",
    "\n",
    "scorer.score_slices(\n",
    "    S=slice_test, \n",
    "    golds=y_test, \n",
    "    preds=preds_test, \n",
    "    probs=probs_test, \n",
    "    as_dataframe=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the model within a selected slice\n",
    "\n",
    "The last example of using the `snorkel` library is optimizing a predictive model for a selected slice. The general idea is to try to learn a better representation of the objects in the selected slice. Of course, an alternative could be oversampling from the slice, but with a large number of slices, manually managing object weights could become very difficult. In the example below, we will use the `SliceAwareClassifier` class which is a wrapper for the base classifier. To define it, we need:\n",
    "\n",
    "- base classifier (`base_architecture`)\n",
    "- output size of the classifier (`head_dim`)\n",
    "- slice names on which the classifier should be optimized (`slice_names`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from snorkel.slicing import SliceAwareClassifier\n",
    "from torch import nn\n",
    "\n",
    "NUM_LAYERS = 4\n",
    "HIDDEN_DIM = X_train.shape[1]\n",
    "\n",
    "layers = []\n",
    "for _ in range(NUM_LAYERS):\n",
    "    layers.extend([nn.Linear(HIDDEN_DIM, HIDDEN_DIM), nn.ReLU()])\n",
    "model = nn.Sequential(*layers)\n",
    "\n",
    "slice_model = SliceAwareClassifier(\n",
    "    base_architecture=model,\n",
    "    head_dim=HIDDEN_DIM,\n",
    "    slice_names=slice_names,\n",
    "    scorer=scorer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applier = PandasSFApplier(sfs)\n",
    "S_train = applier.apply(df_train)\n",
    "S_test = applier.apply(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the model training process, we will be loading batches into the model. It is therefore necessary to add meta-information to each batch about the slices to which individual objects belong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.classification.data import DictDataset, DictDataLoader\n",
    "\n",
    "def create_dict_dataloader(X, Y, split, **kwargs):\n",
    "    \"\"\"Create a DictDataLoader for bag-of-words features.\"\"\"\n",
    "    ds = DictDataset.from_tensors(torch.FloatTensor(X), torch.LongTensor(Y), split)\n",
    "    return DictDataLoader(ds, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dl = create_dict_dataloader(X_train, y_train, \"train\")\n",
    "train_dl_slice = slice_model.make_slice_dataloader(\n",
    "    train_dl.dataset, S_train, shuffle=True, batch_size=BATCH_SIZE\n",
    ")\n",
    "test_dl = create_dict_dataloader(X_test, y_test, \"test\")\n",
    "test_dl_slice = slice_model.make_slice_dataloader(\n",
    "    test_dl.dataset, S_test, shuffle=False, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have all the puzzle pieces ready and we can start optimizing the model on selected slices. We will use the `Trainer` class for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.classification import Trainer\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "trainer = Trainer(n_epochs=NUM_EPOCHS, lr=1e-4, progress_bar=True)\n",
    "trainer.fit(slice_model, [train_dl_slice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_model.score_slices([test_dl_slice], as_dataframe=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming functions\n",
    "\n",
    "The idea of a transforming function is to perform an atomic transformation of an instance. For data that is an image, typical transformations include cropping, rotating, and changing the color palette. For text data, you can replace words with synonyms, substitute named entities, cut random pieces of text, etc. In the following example we will find types of named entities occurring in the text, and then prepare a simple transformer that will randomly replace occurrences of the `PERSON` entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:53:41.946357Z",
     "start_time": "2021-05-20T12:53:31.722008Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "for doc in nlp.pipe(df_train.text.sample(frac=0.05)):\n",
    "    print(f\"Entities: {[(e.text, e.label_) for e in doc.ents]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:55:07.952478Z",
     "start_time": "2021-05-20T12:54:58.669233Z"
    }
   },
   "outputs": [],
   "source": [
    "person_entities = []\n",
    "\n",
    "for doc in nlp.pipe(df_train.text.sample(frac=0.05)):\n",
    "    for e in doc.ents:\n",
    "        if e.label_ == 'PERSON':\n",
    "            person_entities.append(e.text)\n",
    "        \n",
    "person_entities[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T13:00:26.481298Z",
     "start_time": "2021-05-20T13:00:25.034300Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.augmentation import transformation_function\n",
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "\n",
    "spacy = SpacyPreprocessor(text_field=\"text\", doc_field=\"doc\", memoize=True)\n",
    "\n",
    "@transformation_function(pre=[spacy])\n",
    "def random_person_ner(sms):\n",
    "    person_ners = [e.text for e in sms.doc.ents if e.label_ == 'PERSON']\n",
    "    \n",
    "    if person_ners:\n",
    "        person_to_replace = np.random.choice(person_ners)\n",
    "        person_to_add = np.random.choice(person_entities)\n",
    "        sms.text = sms.text.replace(person_to_replace, person_to_add)\n",
    "    return sms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of transformation could be using WordNet to find synonyms for words. However, this requires downloading a corpus of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:58:13.780441Z",
     "start_time": "2021-05-20T12:58:13.378646Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:58:46.154656Z",
     "start_time": "2021-05-20T12:58:46.149076Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_synonym(word):\n",
    "    \n",
    "    synsets = wordnet.synsets(word)\n",
    "    \n",
    "    if synsets:\n",
    "        words = [lemma.name() for lemma in synsets[0].lemmas()]\n",
    "        \n",
    "        return np.random.choice([w.replace(\"_\", \" \") for w in words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:59:30.754714Z",
     "start_time": "2021-05-20T12:59:30.747601Z"
    }
   },
   "outputs": [],
   "source": [
    "@transformation_function()\n",
    "def replace_words_with_synonym(sms, num_replacements=5):\n",
    "\n",
    "    words = sms.text.split()\n",
    "    \n",
    "    for _ in range(num_replacements):\n",
    "        word_idx = np.random.choice(range(len(words)))\n",
    "        synonym = get_synonym(words[word_idx])\n",
    "        if synonym:\n",
    "            words[word_idx] = synonym\n",
    "        \n",
    "    sms.text = ' '.join(words)\n",
    "    return sms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compare the original text message content with the transformed versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:59:48.095751Z",
     "start_time": "2021-05-20T12:59:48.085948Z"
    }
   },
   "outputs": [],
   "source": [
    "# source: https://github.com/snorkel-team/snorkel-tutorials/blob/master/spam/utils.py\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def preview_tfs(df, tfs):\n",
    "    transformed_examples = []\n",
    "    for f in tfs:\n",
    "        for i, row in df.iterrows():\n",
    "            transformed_or_none = f(row)\n",
    "            # If TF returned a transformed example, record it in dict and move to next TF.\n",
    "            if transformed_or_none is not None:\n",
    "                transformed_examples.append(\n",
    "                    OrderedDict(\n",
    "                        {\n",
    "                            \"TF Name\": f.name,\n",
    "                            \"Original Text\": row.text,\n",
    "                            \"Transformed Text\": transformed_or_none.text,\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "    return pd.DataFrame(transformed_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T13:02:44.255054Z",
     "start_time": "2021-05-20T13:02:12.492372Z"
    }
   },
   "outputs": [],
   "source": [
    "tfs = [random_person_ner, replace_words_with_synonym]\n",
    "\n",
    "df_transformed = preview_tfs(df_train.sample(frac=0.1), tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T13:02:45.345938Z",
     "start_time": "2021-05-20T13:02:45.331823Z"
    }
   },
   "outputs": [],
   "source": [
    "df_transformed[df_transformed['Original Text'] != df_transformed['Transformed Text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying transforming functions requires some policy defining the order and number of transformations. In the example below, two transformation functions are drawn at random and this sequence of two functions is applied twice to each data point. As a result, we triple the size of the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T13:04:35.556920Z",
     "start_time": "2021-05-20T13:04:05.019325Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.augmentation import RandomPolicy, PandasTFApplier\n",
    "\n",
    "random_policy = RandomPolicy(len(tfs), sequence_length=2, n_per_original=2, keep_original=True)\n",
    "\n",
    "tf_applier = PandasTFApplier(tfs, random_policy)\n",
    "\n",
    "df_train_sample = df_train.sample(frac=0.1)\n",
    "df_train_augmented = tf_applier.apply(df_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T13:04:40.337473Z",
     "start_time": "2021-05-20T13:04:40.332345Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_sample.shape, df_train_augmented.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assignment\n",
    "\n",
    "Modify the transforming function ``replace_words_with_synonym()`` so that you can restrict the replacement of words with synonyms only for specific parts of speech (e.g., replace only nouns or verbs)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
