{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afd27107",
   "metadata": {},
   "source": [
    "# Data augmentation\n",
    "\n",
    "Data augmentation is the process of altering original data items in a structured way to produce similar data items that differ from the original data item in a precisely defined way. Data augmentation techniques can be applied to textual data, image data, and audio data. In this exercise we will focus on augmentation techniques for text and images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf026da",
   "metadata": {},
   "source": [
    "## Augmentation of text\n",
    "\n",
    "Data augmentation for text can be performed on three levels:\n",
    "- character modifications\n",
    "- word modifications\n",
    "- sentence modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abffb6a",
   "metadata": {},
   "source": [
    "### Character-level augmentation\n",
    "\n",
    "There are two primary reasons for doing character-level augmentation:\n",
    "- to simulate typing errors\n",
    "- to simulate OCR errors\n",
    "\n",
    "Below we start with a simple example of augmentation that tries to address these two cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31607cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:12:42.377897Z",
     "start_time": "2021-06-11T12:12:42.262391Z"
    }
   },
   "outputs": [],
   "source": [
    "from nlpaug.augmenter.char import KeyboardAug\n",
    "\n",
    "_input = 'Poznan is a medium-sized city on the banks of the Warta river'\n",
    "_output = KeyboardAug().augment(_input)\n",
    "\n",
    "print(f\"{_input}\\n{_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4734f2a8",
   "metadata": {},
   "source": [
    "The `nlpaug` library allows to produce multiple augmentations for a single input, as well as processing a list of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4655ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:13:22.318538Z",
     "start_time": "2021-06-11T12:13:22.304009Z"
    }
   },
   "outputs": [],
   "source": [
    "_input = 'To be or not to be, this is the question'\n",
    "_output = KeyboardAug().augment(_input, n=5)\n",
    "\n",
    "print(f\"{_input}\")\n",
    "\n",
    "for result in _output:\n",
    "    print(f\"{result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e4c01e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:13:22.882600Z",
     "start_time": "2021-06-11T12:13:22.870615Z"
    }
   },
   "outputs": [],
   "source": [
    "_input = [\n",
    "    'This is the first sentence',\n",
    "    'This is the second sentence',\n",
    "    'This is the third sentence',\n",
    "]\n",
    "_output = KeyboardAug().augment(_input)\n",
    "\n",
    "for original, transformed in zip(_input, _output):\n",
    "    print(f\"{original}\\n{transformed}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a16908a",
   "metadata": {},
   "source": [
    "If the training data for the ML model have been extracted via OCR from images, there is probably a very specific bias in the data. For instance, the lower case letter `l` can be easily mistaken for the digit `1` or for the upper case `I`. Similarly, `9` and `g` are often confused as well as `0` and `O`. `OCRAug` class simulates these types of mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6c2d01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:15:47.467803Z",
     "start_time": "2021-06-11T12:15:47.462012Z"
    }
   },
   "outputs": [],
   "source": [
    "from nlpaug.augmenter.char import OcrAug\n",
    "\n",
    "_input = 'This bottle has the volume of 10l and it weights 230g'\n",
    "_output = OcrAug().augment(_input)\n",
    "\n",
    "print(f\"{_input}\\n{_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4626c67d",
   "metadata": {},
   "source": [
    "Finally, `nlpaug` allows to introduce random insertions, swaps, and deletions of characters from the source text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571a219c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:17:27.764833Z",
     "start_time": "2021-06-11T12:17:27.746733Z"
    }
   },
   "outputs": [],
   "source": [
    "from nlpaug.augmenter.char import RandomCharAug\n",
    "from nlpaug.flow import Action\n",
    "\n",
    "_input = 'Imagine all the people living life of peace'\n",
    "\n",
    "_output_ins = RandomCharAug(action=Action.INSERT).augment(_input)\n",
    "_output_sub = RandomCharAug(action='substitute').augment(_input)\n",
    "_output_swp = RandomCharAug(action='swap').augment(_input)\n",
    "_output_del = RandomCharAug(action='delete').augment(_input)\n",
    "\n",
    "print(f\"{_input}\")\n",
    "print(f\"{_output_ins}\\n{_output_sub}\\n{_output_swp}\\n{_output_del}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9d8e5d",
   "metadata": {},
   "source": [
    "### Word-level augmentation\n",
    "\n",
    "Word-level augmentation can be used in many scenarios depending on the characteristics of the training dataset. Examples of augmentations include:\n",
    "\n",
    "- substituting a word for an alternative spelling\n",
    "- finding a synonym of a word based on word embeddings\n",
    "- finding a synonym of a word based on TF-IDF\n",
    "- finding a synonym of a word based on contextual word embeddings\n",
    "- finding a synonym of a word based on a dictionary\n",
    "- randomly splitting words\n",
    "- substituting a word for its antonym\n",
    "\n",
    "Below we will see some of the above scenarios in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7e8e63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:20:50.420592Z",
     "start_time": "2021-06-11T12:20:50.412636Z"
    }
   },
   "outputs": [],
   "source": [
    "from nlpaug.augmenter.word import SpellingAug\n",
    "\n",
    "_input = 'In a hole in the ground there lived a hobbit'\n",
    "\n",
    "_output = SpellingAug().augment(_input, n=3)\n",
    "\n",
    "print(f\"{_input}\")\n",
    "\n",
    "for result in _output:\n",
    "    print(f\"{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4749345",
   "metadata": {},
   "source": [
    "The next example requires you to download the pre-trained word embeddings. We are using `fasttext` Englishl embeddings trained on the Wikipedia corpus. You can download the model from [https://fasttext.cc/docs/en/english-vectors.html](https://fasttext.cc/docs/en/english-vectors.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709eb6b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:34:11.099142Z",
     "start_time": "2021-06-11T12:34:10.294790Z"
    }
   },
   "outputs": [],
   "source": [
    "from nlpaug.augmenter.word import WordEmbsAug\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "augmenter = WordEmbsAug(model_type='fasttext', \n",
    "                        model_path='wiki-news-300d-1M.vec',\n",
    "                        action='substitute',\n",
    "                        top_k=50\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55ce60d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:34:11.099142Z",
     "start_time": "2021-06-11T12:34:10.294790Z"
    }
   },
   "outputs": [],
   "source": [
    "_input = 'To be or not to be this is the question'\n",
    "_output = augmenter.augment(_input)\n",
    "\n",
    "print(f\"{_input}\\n{_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e34269a",
   "metadata": {},
   "source": [
    "`FastText` or `word2vec` embeddings are static, a given word always receives the same vector, independent of the textual context. If you want to use contextual embeddings, you have to use the BERT family of language models (`bert-base-uncased`, `distilbert-base-uncased`, `roberta-base`, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4330d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:37:29.700009Z",
     "start_time": "2021-06-11T12:37:18.159804Z"
    }
   },
   "outputs": [],
   "source": [
    "from nlpaug.augmenter.word import ContextualWordEmbsAug\n",
    "\n",
    "augmenter = ContextualWordEmbsAug(\n",
    "    model_path='roberta-base', \n",
    "    action='substitute'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ad765",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:37:29.700009Z",
     "start_time": "2021-06-11T12:37:18.159804Z"
    }
   },
   "outputs": [],
   "source": [
    "_input = 'In a hole in the ground there lived a hobbit'\n",
    "_output = augmenter.augment(_input, n=10)\n",
    "\n",
    "print(f\"{_input}\\n{_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42978422",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:42:25.205956Z",
     "start_time": "2021-06-11T12:42:25.191168Z"
    }
   },
   "outputs": [],
   "source": [
    "from nlpaug.augmenter.word import SynonymAug\n",
    "\n",
    "_input = 'In the hole in the ground there lived a little hobbit'\n",
    "_output = SynonymAug(aug_src='wordnet').augment(_input)\n",
    "\n",
    "print(f\"{_input}\\n{_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f095d639",
   "metadata": {},
   "source": [
    "An interesting option is to use antonyms for generating alternatives for the training data. Here is a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e546afc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:47:26.142992Z",
     "start_time": "2021-06-11T12:47:26.129221Z"
    }
   },
   "outputs": [],
   "source": [
    "from nlpaug.augmenter.word import AntonymAug\n",
    "\n",
    "_input = 'I really loved the movie it was great'\n",
    "_output = AntonymAug().augment(_input)\n",
    "\n",
    "print(f\"{_input}\\n{_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ff0f66",
   "metadata": {},
   "source": [
    "Finally, `RandomWordAug` allows us to randomly swap, delete and crop words from the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdb02f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:47:39.203292Z",
     "start_time": "2021-06-11T12:47:39.189002Z"
    }
   },
   "outputs": [],
   "source": [
    "from nlpaug.augmenter.word import RandomWordAug\n",
    "\n",
    "_input = 'Once upon a time there was a little bird'\n",
    "\n",
    "_output_swp = RandomWordAug(action='swap').augment(_input)\n",
    "_output_del = RandomWordAug(action='delete').augment(_input)\n",
    "_output_crp = RandomWordAug(action='crop').augment(_input)\n",
    "\n",
    "print(f\"{_input}\")\n",
    "print(f\"{_output_swp}\\n{_output_del}\\n{_output_crp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b28c28",
   "metadata": {},
   "source": [
    "There is also a word-level augmenter which performs random splitting of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a599f10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:47:54.184494Z",
     "start_time": "2021-06-11T12:47:54.171718Z"
    }
   },
   "outputs": [],
   "source": [
    "from nlpaug.augmenter.word import SplitAug\n",
    "\n",
    "_input = 'In a hole in the ground there lived a hobbit'\n",
    "_output = SplitAug().augment(_input)\n",
    "\n",
    "print(f\"{_input}\\n{_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04abed5a",
   "metadata": {},
   "source": [
    "Finally, if you want to augment only specific words (for which you know precisely the alternatives), you can use the `ReservedAug`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5979322",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:49:54.548868Z",
     "start_time": "2021-06-11T12:49:54.541222Z"
    }
   },
   "outputs": [],
   "source": [
    "from nlpaug.augmenter.word import ReservedAug\n",
    "\n",
    "reserved_words = [\n",
    "    ['FW', 'Fwd', 'F/D', 'Forward'],\n",
    "    ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "]\n",
    "\n",
    "_input = 'Fwd: Sales report for Q1'\n",
    "_output = ReservedAug(reserved_tokens=reserved_words).augment(_input)\n",
    "\n",
    "print(f\"{_input}\\n{_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e66b5b8",
   "metadata": {},
   "source": [
    "### Sentence-level augmentation\n",
    "\n",
    "Sentence-level augmentation uses state-of-the-art contextual word embeddings created by the BERT family of language models. In order to use these models you have to install the following dependencies:\n",
    "\n",
    "```bash\n",
    "bash$ pip install torch>=1.6.0 transformers>=4.0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f00188e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:51:42.551253Z",
     "start_time": "2021-06-11T12:51:32.043540Z"
    }
   },
   "outputs": [],
   "source": [
    "from nlpaug.augmenter.sentence import ContextualWordEmbsForSentenceAug\n",
    "\n",
    "augmenter = ContextualWordEmbsForSentenceAug(\n",
    "    model_path='gpt2' , \n",
    "    min_length=10,\n",
    "    temperature=0.5, \n",
    "    top_k=50,\n",
    "    top_p=0.7\n",
    ")\n",
    "\n",
    "_input = 'I really enjoyed the wine'\n",
    "_output = augmenter.augment(_input)\n",
    "\n",
    "print(f\"{_input}\\n{_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f114897d",
   "metadata": {},
   "source": [
    "### Sequences of augmentations\n",
    "\n",
    "`nlpaug` allows you to define two modes of sequential augmentations:\n",
    "- provide a list of augmentations to be applied sequentially\n",
    "- provide a list of augmentations which will randomly be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c0614a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:52:34.747017Z",
     "start_time": "2021-06-11T12:52:34.726341Z"
    }
   },
   "outputs": [],
   "source": [
    "from nlpaug.flow import Sequential, Sometimes\n",
    "from nlpaug.util import Action\n",
    "from nlpaug.augmenter.word import RandomWordAug\n",
    "from nlpaug.augmenter.char import RandomCharAug\n",
    "\n",
    "_flow = Sequential(\n",
    "    [\n",
    "    RandomCharAug(action=Action.INSERT),\n",
    "    RandomCharAug(action=Action.SWAP),\n",
    "    RandomWordAug(action=Action.DELETE),\n",
    "    ]\n",
    ")\n",
    "\n",
    "_input = 'In a hole in the ground there lived a hobbit'\n",
    "_output = _flow.augment(_input)\n",
    "\n",
    "print(f\"{_input}\\n{_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee376828",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Use the [IMDB Dataset](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) to create a sample of 100 movie reviews. Then, create a sequence of transformations that are relevant for training the sentiment model. Assume that the reviews have been manually typed on a keyboard. Include character-level, word-level and sentence-level augmentations. In particular, try to augment movie reviews with relevant synonyms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c09004e",
   "metadata": {},
   "source": [
    "## Augmentation for images\n",
    "\n",
    "We will experiment with `Augmentor` library to perform random augmentations of images. `Augmentor` allows you to define a _pipeline_ of augmentations. Each augmentation has a parameter `probability` which defines, how probable it is that the augmentation will be applied. \n",
    "\n",
    "For this part of the exercise we will use the [Face Mask Detection dataset](https://www.kaggle.com/sshikamaru/face-mask-detection) from Kaggle Datasets.\n",
    "\n",
    "After downloading and unpacking the dataset, please create an additional directory with a single image for demonstration purposes.\n",
    "\n",
    "```bash\n",
    "bash$ mkdir images/example\n",
    "bash$ cp images/train/smartmi-3pcs-filter-mask-pm25-haze-dustproof-mask-with-vent_jpg.rf.c5b0c5b7666032c5b4634740eafde234.jpg images/example/image.jpg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ebe7a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:54:48.786675Z",
     "start_time": "2021-06-11T12:54:48.667824Z"
    }
   },
   "outputs": [],
   "source": [
    "import Augmentor\n",
    "\n",
    "path_to_files = 'images/example/'\n",
    "\n",
    "p = Augmentor.Pipeline(path_to_files)\n",
    "p.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e31f27f",
   "metadata": {},
   "source": [
    "We will start with a very simple horizontal mirror flip of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5222d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:55:24.207644Z",
     "start_time": "2021-06-11T12:55:24.159148Z"
    }
   },
   "outputs": [],
   "source": [
    "p.flip_left_right(probability=0.5)\n",
    "\n",
    "p.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3065196c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:55:48.179204Z",
     "start_time": "2021-06-11T12:55:48.161336Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display, HTML\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "def make_html(image):\n",
    "     return '<img src=\"{}\" style=\"display:inline;margin:1px;width:100px\"/>'.format(image)\n",
    "    \n",
    "original_image = 'images/example/image.jpg'\n",
    "transformed_images = glob('images/example/output/*.jpg')\n",
    "\n",
    "display(Image(filename=original_image, width=100, height=100))\n",
    "\n",
    "html_images = ''.join([make_html(x) for x in transformed_images])\n",
    "display(HTML(html_images))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82779b4",
   "metadata": {},
   "source": [
    "Next, we will illustrate some more transformations available in the `Augmentor` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080ac2e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:56:58.445622Z",
     "start_time": "2021-06-11T12:56:57.939424Z"
    }
   },
   "outputs": [],
   "source": [
    "!rm images/example/output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a7db67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:57:06.952575Z",
     "start_time": "2021-06-11T12:57:06.839050Z"
    }
   },
   "outputs": [],
   "source": [
    "# rotation\n",
    "\n",
    "path_to_files = 'images/example'\n",
    "\n",
    "p = Augmentor.Pipeline(path_to_files)\n",
    "\n",
    "# p.rotate(probability=1.0, max_left_rotation=25, max_right_rotation=25)\n",
    "# p.rotate180(probability=0.5)\n",
    "p.rotate90(probability=0.5)\n",
    "# p.rotate_random_90(probability=0.5)\n",
    "\n",
    "p.sample(10)\n",
    "\n",
    "original_image = 'images/example/image.jpg'\n",
    "transformed_images = glob('images/example/output/*.jpg')\n",
    "\n",
    "display(Image(filename=original_image, width=100, height=100))\n",
    "\n",
    "html_images = ''.join([make_html(x) for x in transformed_images])\n",
    "display(HTML(html_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f04bbfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:57:43.231943Z",
     "start_time": "2021-06-11T12:57:42.733001Z"
    }
   },
   "outputs": [],
   "source": [
    "!rm images/example/output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146f3e11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:57:57.330807Z",
     "start_time": "2021-06-11T12:57:57.189912Z"
    }
   },
   "outputs": [],
   "source": [
    "# zooming\n",
    "\n",
    "path_to_files = 'images/example'\n",
    "\n",
    "p = Augmentor.Pipeline(path_to_files)\n",
    "\n",
    "# p.zoom(probability=0.9, min_factor=1.1, max_factor=3.0)\n",
    "p.zoom_random(probability=0.9, percentage_area=0.5)\n",
    "\n",
    "p.sample(10)\n",
    "\n",
    "original_image = 'images/example/image.jpg'\n",
    "transformed_images = glob('images/example/output/*.jpg')\n",
    "\n",
    "display(Image(filename=original_image, width=100, height=100))\n",
    "\n",
    "html_images = ''.join([make_html(x) for x in transformed_images])\n",
    "display(HTML(html_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e456ff43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:58:48.426314Z",
     "start_time": "2021-06-11T12:58:47.967460Z"
    }
   },
   "outputs": [],
   "source": [
    "!rm images/example/output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5195f402",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:59:14.652044Z",
     "start_time": "2021-06-11T12:59:14.467100Z"
    }
   },
   "outputs": [],
   "source": [
    "# perspective skewing\n",
    "\n",
    "path_to_files = 'images/example'\n",
    "\n",
    "p = Augmentor.Pipeline(path_to_files)\n",
    "\n",
    "p.skew_tilt(probability=0.9)\n",
    "# p.skew_left_right(probability=0.5)\n",
    "# p.skew_top_bottom(probability=0.5)\n",
    "# p.skew_corner(probability=0.5)\n",
    "\n",
    "p.sample(10)\n",
    "\n",
    "original_image = 'images/example/image.jpg'\n",
    "transformed_images = glob('images/example/output/*.jpg')\n",
    "\n",
    "display(Image(filename=original_image, width=100, height=100))\n",
    "\n",
    "html_images = ''.join([make_html(x) for x in transformed_images])\n",
    "display(HTML(html_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19698e5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:00:52.870772Z",
     "start_time": "2021-06-11T13:00:52.378358Z"
    }
   },
   "outputs": [],
   "source": [
    "!rm images/example/output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100c596b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:01:16.178744Z",
     "start_time": "2021-06-11T13:01:16.085169Z"
    }
   },
   "outputs": [],
   "source": [
    "# elastic distortions\n",
    "\n",
    "path_to_files = 'images/example'\n",
    "\n",
    "p = Augmentor.Pipeline(path_to_files)\n",
    "\n",
    "# p.random_distortion(probability=0.9, grid_width=50, grid_height=50, magnitude=10)\n",
    "# p.random_color(probability=0.5, min_factor=0.1, max_factor=0.5)\n",
    "# p.random_contrast(probability=0.5, min_factor=0.1, max_factor=0.5)\n",
    "# p.random_erasing(probability=0.5, rectangle_area=0.25)\n",
    "p.random_brightness(probability=0.5, min_factor=0.1, max_factor=0.5)\n",
    "\n",
    "p.sample(10)\n",
    "\n",
    "original_image = 'images/example/image.jpg'\n",
    "transformed_images = glob('images/example/output/*.jpg')\n",
    "\n",
    "display(Image(filename=original_image, width=100, height=100))\n",
    "\n",
    "html_images = ''.join([make_html(x) for x in transformed_images])\n",
    "display(HTML(html_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96403b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:01:22.858573Z",
     "start_time": "2021-06-11T13:01:22.359754Z"
    }
   },
   "outputs": [],
   "source": [
    "!rm images/example/output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5d4dd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:01:25.777829Z",
     "start_time": "2021-06-11T13:01:25.590350Z"
    }
   },
   "outputs": [],
   "source": [
    "# shearing\n",
    "\n",
    "path_to_files = 'images/example'\n",
    "\n",
    "p = Augmentor.Pipeline(path_to_files)\n",
    "\n",
    "p.shear(probability=0.5, max_shear_left=25, max_shear_right=25)\n",
    "\n",
    "p.sample(10)\n",
    "\n",
    "original_image = 'images/example/image.jpg'\n",
    "transformed_images = glob('images/example/output/*.jpg')\n",
    "\n",
    "display(Image(filename=original_image, width=100, height=100))\n",
    "\n",
    "html_images = ''.join([make_html(x) for x in transformed_images])\n",
    "display(HTML(html_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e50642e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:02:45.581132Z",
     "start_time": "2021-06-11T13:02:45.102207Z"
    }
   },
   "outputs": [],
   "source": [
    "!rm images/example/output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd84a03e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:02:55.326311Z",
     "start_time": "2021-06-11T13:02:55.239314Z"
    }
   },
   "outputs": [],
   "source": [
    "# cropping\n",
    "\n",
    "path_to_files = 'images/example'\n",
    "\n",
    "p = Augmentor.Pipeline(path_to_files)\n",
    "\n",
    "# p.crop_centre(probability=0.9, percentage_area=0.75)\n",
    "# p.crop_by_size(probability=0.5, width=100, height=100)\n",
    "p.crop_random(probability=0.9, percentage_area=0.5)\n",
    "\n",
    "p.sample(10)\n",
    "\n",
    "original_image = 'images/example/image.jpg'\n",
    "transformed_images = glob('images/example/output/*.jpg')\n",
    "\n",
    "display(Image(filename=original_image, width=100, height=100))\n",
    "\n",
    "html_images = ''.join([make_html(x) for x in transformed_images])\n",
    "display(HTML(html_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3784c0b4",
   "metadata": {},
   "source": [
    "If you wish to process each image in the pipeline exactly once, use `process()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a53398",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:04:25.852345Z",
     "start_time": "2021-06-11T13:04:25.367579Z"
    }
   },
   "outputs": [],
   "source": [
    "!rm images/example/output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be11702",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:04:32.436116Z",
     "start_time": "2021-06-11T13:04:32.409991Z"
    }
   },
   "outputs": [],
   "source": [
    "# resizing\n",
    "\n",
    "def make_html_original_size(image):\n",
    "     return '<img src=\"{}\" style=\"display:inline;margin:1px\"/>'.format(image)\n",
    "    \n",
    "path_to_files = 'images/example'\n",
    "\n",
    "p = Augmentor.Pipeline(path_to_files)\n",
    "\n",
    "p.resize(probability=1.0, width=100, height=100)\n",
    "\n",
    "p.process()\n",
    "\n",
    "original_image = 'images/example/image.jpg'\n",
    "transformed_images = glob('images/example/output/*.jpg')\n",
    "\n",
    "display(Image(filename=original_image))\n",
    "\n",
    "html_images = ''.join([make_html_original_size(x) for x in transformed_images])\n",
    "display(HTML(html_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32dfdbb",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Use the `images/train` folder to create a set of transformations of images. When designing the augmentation pipeline, consider the main aim of the augmentation: to improve the model for face mask detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d9ab62",
   "metadata": {},
   "source": [
    "## Testing NLP models with Checklist\n",
    "\n",
    "Checklist is an interesting library which allows you to perform behavioral testing of NLP models. Behavioral tests do not verify the internal structure of the model, they simply verify the behavior (compare output to expected output) of the model. Checklist allows to do two basic tasks:\n",
    "\n",
    "- create artificial test instances (examples)\n",
    "- apply augmentations and distortions to test instances\n",
    "\n",
    "We begin by using a *template* to create some test data. The template may simply insert words from a dictionary into a placeholder, or it can use a special token `{mask}` to insert arbitrary tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cecace1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a recipe for a a good dinner.',\n",
       " 'This is a recipe for a an amazing dinner.',\n",
       " 'This is a recipe for a a fantastic dinner.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import checklist\n",
    "from checklist.editor import Editor\n",
    "\n",
    "pos_adjectives = [\"good\", \"great\", \"amazing\", \"super\", \"fantastic\"]\n",
    "\n",
    "e = Editor()\n",
    "\n",
    "e.template(\"This is a recipe for a {a:pos} dinner.\", pos=pos_adjectives, nsamples=3).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a1cb4df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikolaj/venv/python39/lib/python3.9/site-packages/checklist/text_generation.py:171: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  to_pred = torch.tensor(to_pred, device=self.device).to(torch.int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Apple is not a great recipe for dinner.',\n",
       " 'Music is not an amazing recipe for dinner.',\n",
       " 'Today is not a fantastic recipe for dinner.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.template(\"{mask} is not {a:pos} recipe for dinner.\", pos=pos_adjectives, nsamples=3).data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09ea4fb",
   "metadata": {},
   "source": [
    "We will now create a larger test suite of generic phrases with affective adjectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4d48e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('For holidays gardening is a a realistic option.', 0),\n",
       " ('For holidays breastfeeding is a a recommended option.', 0),\n",
       " ('For holidays vaping is a a feasible option.', 0),\n",
       " ('For holidays skiing is a an exciting option.', 0),\n",
       " ('For holidays sailing is a a best option.', 0),\n",
       " ('For holidays volunteering is a an effective option.', 0),\n",
       " ('For holidays there is a an acceptable option.', 0),\n",
       " ('For holidays rent is a an effective option.', 0),\n",
       " ('For holidays football is a an economical option.', 0),\n",
       " ('For holidays fishing is a an affordable option.', 0),\n",
       " ('For holidays printing is a an inexpensive option.', 0),\n",
       " ('For holidays Pizza is a an effective option.', 0),\n",
       " ('For holidays sailing is a an appealing option.', 0),\n",
       " ('For holidays cider is a an ideal option.', 0),\n",
       " ('For holidays VPN is a an appropriate option.', 0),\n",
       " ('For holidays bacon is a a good option.', 0),\n",
       " ('For holidays chicken is a an exciting option.', 0),\n",
       " ('For holidays Android is a an ideal option.', 0),\n",
       " ('For holidays printing is a an ideal option.', 0),\n",
       " ('For holidays Airbnb is a a feasible option.', 0),\n",
       " ('For holidays pizza is a a realistic option.', 0),\n",
       " ('For holidays baking is a a good option.', 0),\n",
       " ('For holidays Minecraft is a an appropriate option.', 0),\n",
       " ('For holidays cycling is a an easy option.', 0),\n",
       " ('For holidays Android is a an exciting option.', 0),\n",
       " ('For holidays food is a an excellent option.', 0),\n",
       " ('For holidays that is a an obvious option.', 0),\n",
       " ('For holidays volunteering is a an affordable option.', 0),\n",
       " ('For holidays these is a an easy option.', 0),\n",
       " ('For holidays crowdfunding is a a feasible option.', 0),\n",
       " ('For holidays fishing is a an attractive option.', 0),\n",
       " ('For holidays Pizza is a a best option.', 0),\n",
       " ('For holidays cake is a an appropriate option.', 0),\n",
       " ('For holidays renting is a an attractive option.', 0),\n",
       " ('For holidays smoking is a an appealing option.', 0),\n",
       " ('For holidays wifi is a an appropriate option.', 0),\n",
       " ('For holidays turkey is a an economical option.', 0),\n",
       " ('For holidays Pizza is a a recommended option.', 0),\n",
       " ('For holidays insurance is a a great option.', 0),\n",
       " ('For holidays hunting is a an exciting option.', 0),\n",
       " ('For holidays here is a an excellent option.', 0),\n",
       " ('For holidays that is a an ideal option.', 0),\n",
       " ('For holidays crowdfunding is a an effective option.', 0),\n",
       " ('For holidays knitting is a an obvious option.', 0),\n",
       " ('For holidays shopping is a a good option.', 0),\n",
       " ('For holidays cannabis is a an appealing option.', 0),\n",
       " ('For holidays smoking is a an obvious option.', 0),\n",
       " ('For holidays ice is a a healthy option.', 0),\n",
       " ('For holidays Airbnb is a a feasible option.', 0),\n",
       " ('For holidays skiing is a an easy option.', 0),\n",
       " ('For holidays wine is a an ideal option.', 0),\n",
       " ('For holidays bacon is a an attractive option.', 0),\n",
       " ('For holidays chicken is a a feasible option.', 0),\n",
       " ('For holidays these is a an appealing option.', 0),\n",
       " ('For holidays water is a an acceptable option.', 0),\n",
       " ('For holidays bitcoin is a an excellent option.', 0),\n",
       " ('For holidays turkey is a a best option.', 0),\n",
       " ('For holidays travelling is a an easy option.', 0),\n",
       " ('For holidays cannabis is a a good option.', 0),\n",
       " ('For holidays Lego is a a healthy option.', 0),\n",
       " ('For holidays Steam is a an affordable option.', 0),\n",
       " ('For holidays Twitter is a an obvious option.', 0),\n",
       " ('For holidays cannabis is a an obvious option.', 0),\n",
       " ('For holidays travelling is a a feasible option.', 0),\n",
       " ('For holidays biking is a an excellent option.', 0),\n",
       " ('For holidays online is a an affordable option.', 0),\n",
       " ('For holidays golf is a an inexpensive option.', 0),\n",
       " ('For holidays travel is a a good option.', 0),\n",
       " ('For holidays flying is a a realistic option.', 0),\n",
       " ('For holidays online is a an appropriate option.', 0),\n",
       " ('For holidays gold is a an exciting option.', 0),\n",
       " ('For holidays bacon is a an attractive option.', 0),\n",
       " ('For holidays biking is a a good option.', 0),\n",
       " ('For holidays music is a an exciting option.', 0),\n",
       " ('For holidays this is a a best option.', 0),\n",
       " ('For holidays milk is a a best option.', 0),\n",
       " ('For holidays champagne is a a healthy option.', 0),\n",
       " ('For holidays photography is a a recommended option.', 0),\n",
       " ('For holidays Amazon is a an economical option.', 0),\n",
       " ('For holidays these is a a feasible option.', 0),\n",
       " ('For holidays gas is a an exciting option.', 0),\n",
       " ('For holidays DIY is a an easy option.', 0),\n",
       " ('For holidays there is a an economical option.', 0),\n",
       " ('For holidays crowdfunding is a a recommended option.', 0),\n",
       " ('For holidays insurance is a an economical option.', 0),\n",
       " ('For holidays Android is a an appealing option.', 0),\n",
       " ('For holidays printing is a a feasible option.', 0),\n",
       " ('For holidays meditation is a a feasible option.', 0),\n",
       " ('For holidays flying is a an exciting option.', 0),\n",
       " ('For holidays cake is a an appealing option.', 0),\n",
       " ('For holidays sailing is a a great option.', 0),\n",
       " ('For holidays catering is a an effective option.', 0),\n",
       " ('For holidays flying is a a good option.', 0),\n",
       " ('For holidays breastfeeding is a an exciting option.', 0),\n",
       " ('For holidays This is a an affordable option.', 0),\n",
       " ('For holidays Netflix is a an appropriate option.', 0),\n",
       " ('For holidays WiFi is a a feasible option.', 0),\n",
       " ('For holidays meditation is a an affordable option.', 0),\n",
       " ('For holidays Netflix is a a good option.', 0),\n",
       " ('For holidays crowdfunding is a a recommended option.', 0),\n",
       " ('For holidays suicide is a an expensive option.', 1),\n",
       " ('For holidays wine is a a terrible option.', 1),\n",
       " ('For holidays which is a a worst option.', 1),\n",
       " ('For holidays bitcoin is a an expensive option.', 1),\n",
       " ('For holidays austerity is a a terrible option.', 1),\n",
       " ('For holidays turkey is a an expensive option.', 1),\n",
       " ('For holidays tweeting is a a terrible option.', 1),\n",
       " ('For holidays heating is a an unappropriate option.', 1),\n",
       " ('For holidays alcohol is a an unappropriate option.', 1),\n",
       " ('For holidays fasting is a a time-consuming option.', 1),\n",
       " ('For holidays downloading is a an unhealthy option.', 1),\n",
       " ('For holidays camping is a an unappropriate option.', 1),\n",
       " ('For holidays childcare is a a bad option.', 1),\n",
       " ('For holidays gardening is a an unfeasible option.', 1),\n",
       " ('For holidays tweeting is a a time-consuming option.', 1),\n",
       " ('For holidays housing is a a time-consuming option.', 1),\n",
       " ('For holidays blogging is a an unappropriate option.', 1),\n",
       " ('For holidays medication is a an unfeasible option.', 1),\n",
       " ('For holidays dancing is a a time-consuming option.', 1),\n",
       " ('For holidays football is a an unhealthy option.', 1),\n",
       " ('For holidays childcare is a a bad option.', 1),\n",
       " ('For holidays coffee is a an unhealthy option.', 1),\n",
       " ('For holidays that is a an awful option.', 1),\n",
       " ('For holidays fishing is a an awful option.', 1),\n",
       " ('For holidays rent is a a boring option.', 1),\n",
       " ('For holidays driving is a a time-consuming option.', 1),\n",
       " ('For holidays football is a an awful option.', 1),\n",
       " ('For holidays travel is a an unappropriate option.', 1),\n",
       " ('For holidays flying is a a boring option.', 1),\n",
       " ('For holidays heating is a an unfeasible option.', 1),\n",
       " ('For holidays leaving is a a worst option.', 1),\n",
       " ('For holidays Google is a an unhealthy option.', 1),\n",
       " ('For holidays camping is a a time-consuming option.', 1),\n",
       " ('For holidays gardening is a a boring option.', 1),\n",
       " ('For holidays champagne is a a terrible option.', 1),\n",
       " ('For holidays milk is a an awful option.', 1),\n",
       " ('For holidays Skype is a an unhealthy option.', 1),\n",
       " ('For holidays baking is a a terrible option.', 1),\n",
       " ('For holidays freezing is a a bad option.', 1),\n",
       " ('For holidays gardening is a a boring option.', 1),\n",
       " ('For holidays insurance is a a time-consuming option.', 1),\n",
       " ('For holidays gambling is a an expensive option.', 1),\n",
       " ('For holidays sleeping is a a bad option.', 1),\n",
       " ('For holidays flying is a a terrible option.', 1),\n",
       " ('For holidays Google is a a time-consuming option.', 1),\n",
       " ('For holidays cannabis is a an expensive option.', 1),\n",
       " ('For holidays smoking is a an unappropriate option.', 1),\n",
       " ('For holidays marriage is a an unappropriate option.', 1),\n",
       " ('For holidays Skype is a a time-consuming option.', 1),\n",
       " ('For holidays Twitter is a an awful option.', 1),\n",
       " ('For holidays coffee is a an unfeasible option.', 1),\n",
       " ('For holidays turkey is a a time-consuming option.', 1),\n",
       " ('For holidays shopping is a a boring option.', 1),\n",
       " ('For holidays medication is a an unappropriate option.', 1),\n",
       " ('For holidays dancing is a an unappropriate option.', 1),\n",
       " ('For holidays leaving is a a terrible option.', 1),\n",
       " ('For holidays shipping is a a worst option.', 1),\n",
       " ('For holidays renting is a a time-consuming option.', 1),\n",
       " ('For holidays freezing is a a boring option.', 1),\n",
       " ('For holidays nothing is a an unfeasible option.', 1),\n",
       " ('For holidays marriage is a an awful option.', 1),\n",
       " ('For holidays washing is a a bad option.', 1),\n",
       " ('For holidays ransomware is a an expensive option.', 1),\n",
       " ('For holidays drinking is a an unappropriate option.', 1),\n",
       " ('For holidays marriage is a an unappropriate option.', 1),\n",
       " ('For holidays nothing is a a time-consuming option.', 1),\n",
       " ('For holidays petrol is a an unappropriate option.', 1),\n",
       " ('For holidays sex is a a boring option.', 1),\n",
       " ('For holidays Amazon is a a bad option.', 1),\n",
       " ('For holidays food is a an awful option.', 1),\n",
       " ('For holidays voting is a an unappropriate option.', 1),\n",
       " ('For holidays sex is a an unappropriate option.', 1),\n",
       " ('For holidays yoga is a an unhealthy option.', 1),\n",
       " ('For holidays medication is a an unhealthy option.', 1),\n",
       " ('For holidays petrol is a a boring option.', 1),\n",
       " ('For holidays meditation is a a bad option.', 1),\n",
       " ('For holidays this is a an awful option.', 1),\n",
       " ('For holidays abstinence is a an unfeasible option.', 1),\n",
       " ('For holidays Airbnb is a a terrible option.', 1),\n",
       " ('For holidays music is a a worst option.', 1),\n",
       " ('For holidays vaping is a an unfeasible option.', 1),\n",
       " ('For holidays leaving is a an expensive option.', 1),\n",
       " ('For holidays booze is a a worst option.', 1),\n",
       " ('For holidays texting is a an unappropriate option.', 1),\n",
       " ('For holidays alcohol is a an awful option.', 1),\n",
       " ('For holidays Google is a a terrible option.', 1),\n",
       " ('For holidays insurance is a a terrible option.', 1),\n",
       " ('For holidays football is a an unappropriate option.', 1),\n",
       " ('For holidays childcare is a an unappropriate option.', 1),\n",
       " ('For holidays suicide is a a terrible option.', 1),\n",
       " ('For holidays voting is a a time-consuming option.', 1),\n",
       " ('For holidays champagne is a a bad option.', 1),\n",
       " ('For holidays tweeting is a an expensive option.', 1),\n",
       " ('For holidays email is a a bad option.', 1),\n",
       " ('For holidays voting is a an unappropriate option.', 1),\n",
       " ('For holidays bitcoin is a an unhealthy option.', 1),\n",
       " ('For holidays parking is a an unappropriate option.', 1),\n",
       " ('For holidays chocolate is a a time-consuming option.', 1),\n",
       " ('For holidays school is a an awful option.', 1),\n",
       " ('For holidays suicide is a a terrible option.', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "pos = [\n",
    "  \"good\", \"realistic\", \"healthy\", \"attractive\", \"appealing\", \"acceptable\", \n",
    "  \"best\", \"feasible\", \"easy\", \"ideal\", \"affordable\", \"economical\", \"recommended\", \n",
    "  \"exciting\", \"inexpensive\", \"obvious\", \"great\", \"appropriate\", \"effective\", \"excellent\",\n",
    "  ]\n",
    "\n",
    "neg = [\n",
    "  \"bad\", \"unhealthy\", \"expensive\", \"boring\", \"terrible\", \"worst\", \"unfeasible\", \n",
    "  \"unappropriate\", \"awful\", \"time-consuming\",\n",
    "  ]\n",
    "  \n",
    "samples = e.template(\"For holidays {mask} is a {a:pos} option.\", pos=pos, labels=0, save=True, nsamples=100)\n",
    "samples += e.template(\"For holidays {mask} is a {a:neg} option.\", neg=neg, labels=1, save=True, nsamples=100)\n",
    "\n",
    "list(zip(samples.data, samples.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031e4224",
   "metadata": {},
   "source": [
    "For sentiment classification we will use a simple class `TextBlob` from the `textblob` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efc61620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.7, subjectivity=0.6000000000000001)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "TextBlob(\"This pizza was so good one could kill for it\").sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d21de87",
   "metadata": {},
   "source": [
    "We will also create a simple utility function to return positive/negative sentiment probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "727c8137",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15, 0.85]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_proba(inputs):\n",
    "    p1 = np.array([(TextBlob(x).sentiment[0] + 1) / 2.0 for x in inputs]).reshape(-1, 1)\n",
    "    p0 = 1 - p1\n",
    "    \n",
    "    return np.hstack((p0, p1))\n",
    "\n",
    "\n",
    "predict_proba([\"This pizza was so good one could kill for it\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99978cd4",
   "metadata": {},
   "source": [
    "In order to use our probability prediction function we need to pass it through the wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42daae7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1]), array([[0.15, 0.85]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from checklist.pred_wrapper import PredictorWrapper\n",
    "\n",
    "wrapped_pp = PredictorWrapper.wrap_softmax(predict_proba)\n",
    "wrapped_pp([\"This pizza was so good one could kill for it\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d515ef8e",
   "metadata": {},
   "source": [
    "There are three basic types of tests available in `checklist`:\n",
    "\n",
    "- **minimum functionality test** (MFT): uses simple examples to make sure the model can perform a specific task well\n",
    "- **invariance test** (INV): checks if the model prediction stays the same when trivial parts of inputs are slightly changed\n",
    "- **directional expectation test** (DET): checks if the model changes the prediction in the desired direction when parts of the inputs are slightly changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f1b6398",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 200 examples\n"
     ]
    }
   ],
   "source": [
    "from checklist.test_types import MFT, INV, DIR\n",
    "\n",
    "test = MFT(\n",
    "    samples.data,\n",
    "    labels=samples.labels,\n",
    "    name=\"Test negation\",\n",
    "    capability=\"Negation\"\n",
    ")\n",
    "\n",
    "test.run(wrapped_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "652c5756",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:      200\n",
      "Fails (rate):    169 (84.5%)\n",
      "\n",
      "Example fails:\n",
      "1.0 For holidays turkey is a a best option.\n",
      "----\n",
      "0.5 For holidays medication is a an unappropriate option.\n",
      "----\n",
      "0.7 For holidays flying is a an exciting option.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "test.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "892a0ff9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc96b5d301a7403184c2bbdc0e6ea889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TestSummarizer(stats={'npassed': 31, 'nfailed': 169, 'nfiltered': 0}, summarizer={'name': 'Test negation', 'de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test.visual_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edf5199",
   "metadata": {},
   "source": [
    "To perform invariance and directional expectation tests we have to convert the sample data into `spacy` documents. We will use a small set of fictitious reviews of restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0eafc4df-e33f-4337-9911-dd0d8811a67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    \"The falafel was delicious.\",\n",
    "    \"Everything was awful and there were flies in the soup!\",\n",
    "    \"It was the best food me and Kathy in a long time.\",\n",
    "    \"Jimmy hates lettuce...\",\n",
    "    \"This burger joint is not very good?\",\n",
    "    \"Restaurants in Germany are always great!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b201ae3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[The falafel was delicious.,\n",
       " Everything was awful and there were flies in the soup!,\n",
       " It was the best food me and Kathy in a long time.,\n",
       " Jimmy hates lettuce...,\n",
       " This burger joint is not very good?,\n",
       " Restaurants in Germany are always great!]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_data = list(nlp.pipe(data))\n",
    "\n",
    "spacy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f132c05",
   "metadata": {},
   "source": [
    "First, we will verify if the model correctly predicts the sentiment when we perturb the punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34cb1276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The falafel was delicious.', 'The falafel was delicious']\n",
      "['Everything was awful and there were flies in the soup!', 'Everything was awful and there were flies in the soup', 'Everything was awful and there were flies in the soup.']\n",
      "['It was the best food me and Kathy in a long time.', 'It was the best food me and Kathy in a long time']\n",
      "['Jimmy hates lettuce...', 'Jimmy hates lettuce', 'Jimmy hates lettuce.']\n",
      "['This burger joint is not very good?', 'This burger joint is not very good', 'This burger joint is not very good.']\n",
      "['Restaurants in Germany are always great!', 'Restaurants in Germany are always great', 'Restaurants in Germany are always great.']\n"
     ]
    }
   ],
   "source": [
    "from checklist.perturb import Perturb\n",
    "\n",
    "_data = Perturb.perturb(spacy_data, Perturb.punctuation)\n",
    "\n",
    "for row in _data.data:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87c0d6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 16 examples\n",
      "Test cases:      6\n",
      "Fails (rate):    0 (0.0%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba6cb2536c44ace9dc284613d51ffb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TestSummarizer(stats={'npassed': 6, 'nfailed': 0, 'nfiltered': 0}, summarizer={'name': None, 'description': No…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test  = INV(**_data)\n",
    "test.run(wrapped_pp)\n",
    "\n",
    "test.summary()\n",
    "test.visual_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585e4bc8",
   "metadata": {},
   "source": [
    "Next, we will add some typos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d23a6aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The falafel was delicious.', 'The falafel was delicoius.']\n",
      "['Everything was awful and there were flies in the soup!', 'Everything was afwul and there were flies in the soup!']\n",
      "['It was the best food me and Kathy in a long time.', 'It was the best food me and aKthy in a long time.']\n",
      "['Jimmy hates lettuce...', 'Jimmyh ates lettuce...']\n",
      "['This burger joint is not very good?', 'This burgerj oint is not very good?']\n",
      "['Restaurants in Germany are always great!', 'Restaurants in Germanya re always great!']\n"
     ]
    }
   ],
   "source": [
    "_data = Perturb.perturb(data, Perturb.add_typos)\n",
    "\n",
    "for row in _data.data:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "194136d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 12 examples\n",
      "Test cases:      6\n",
      "Fails (rate):    1 (16.7%)\n",
      "\n",
      "Example fails:\n",
      "1.0 The falafel was delicious.\n",
      "0.5 The falafel was delicoius.\n",
      "\n",
      "----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cdc11b0c7384b53b1fd975d93ca351c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TestSummarizer(stats={'npassed': 5, 'nfailed': 1, 'nfiltered': 0}, summarizer={'name': None, 'description': No…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test  = INV(**_data)\n",
    "test.run(wrapped_pp)\n",
    "\n",
    "test.summary()\n",
    "test.visual_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b663aff3",
   "metadata": {},
   "source": [
    "We should verify if the predictions change when we change the proper names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "292331b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The falafel was delicious. []\n",
      "Everything was awful and there were flies in the soup! []\n",
      "It was the best food me and Kathy in a long time. [('Kathy', 'PERSON')]\n",
      "Jimmy hates lettuce... [('Jimmy', 'PERSON')]\n",
      "This burger joint is not very good? []\n",
      "Restaurants in Germany are always great! [('Germany', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "for d in spacy_data:\n",
    "    print(d, [(e.text, e.label_) for e in d.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89350803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It was the best food me and Kathy in a long time.', 'It was the best food me and Elizabeth in a long time.', 'It was the best food me and Destiny in a long time.', 'It was the best food me and Heather in a long time.', 'It was the best food me and Chelsea in a long time.', 'It was the best food me and Katie in a long time.', 'It was the best food me and Victoria in a long time.', 'It was the best food me and Christine in a long time.', 'It was the best food me and Brittany in a long time.', 'It was the best food me and Tracy in a long time.', 'It was the best food me and Shannon in a long time.']\n",
      "['Jimmy hates lettuce...', 'Michael hates lettuce...', 'Thomas hates lettuce...', 'Jeremiah hates lettuce...', 'Isaac hates lettuce...', 'Samuel hates lettuce...', 'Justin hates lettuce...', 'Steven hates lettuce...', 'Timothy hates lettuce...', 'Stephen hates lettuce...', 'Antonio hates lettuce...']\n"
     ]
    }
   ],
   "source": [
    "_data = Perturb.perturb(spacy_data, Perturb.change_names)\n",
    "\n",
    "for row in _data.data:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54daa7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 22 examples\n",
      "Test cases:      2\n",
      "Fails (rate):    0 (0.0%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dffd23215e54a7897f5d44bfae90fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TestSummarizer(stats={'npassed': 2, 'nfailed': 0, 'nfiltered': 0}, summarizer={'name': None, 'description': No…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test  = INV(**_data)\n",
    "test.run(wrapped_pp)\n",
    "\n",
    "test.summary()\n",
    "test.visual_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b8891e",
   "metadata": {},
   "source": [
    "We also do not expect any prediction changes in response to changing the names of cities or countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b87e635f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Restaurants in Germany are always great!', 'Restaurants in Yemen are always great!', 'Restaurants in Bangladesh are always great!', 'Restaurants in Italy are always great!', 'Restaurants in Turkey are always great!', 'Restaurants in Uzbekistan are always great!', 'Restaurants in Argentina are always great!', 'Restaurants in Ethiopia are always great!', 'Restaurants in Vietnam are always great!', 'Restaurants in Peru are always great!', 'Restaurants in Uganda are always great!']\n"
     ]
    }
   ],
   "source": [
    "_data = Perturb.perturb(spacy_data, Perturb.change_location)\n",
    "\n",
    "for row in _data.data:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be12f00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 11 examples\n",
      "Test cases:      1\n",
      "Fails (rate):    0 (0.0%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f09a4c65e44d148eb21db2d740a99e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TestSummarizer(stats={'npassed': 1, 'nfailed': 0, 'nfiltered': 0}, summarizer={'name': None, 'description': No…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test  = INV(**_data)\n",
    "test.run(wrapped_pp)\n",
    "\n",
    "test.summary()\n",
    "test.visual_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239e6366",
   "metadata": {},
   "source": [
    "The last type of test assumes that the change in the input example will trigger the change in the model's prediction and the direction of the change is known. Since we are doing binary sentiment classification only, the direction of change is simply the prediction of the opposite sentiment. We begin by defining a helper function to flag the change of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fb0211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from checklist.expect import Expect\n",
    "\n",
    "def changed_pred(orig_pred, pred, orig_conf, conf, labels=None, meta=None):\n",
    "    return pred != orig_pred\n",
    "\n",
    "\n",
    "expect_fn = Expect.pairwise(changed_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6761c5",
   "metadata": {},
   "source": [
    "As a simple example, we will add negation to our test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b171e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_data = Perturb.perturb(spacy_data, Perturb.add_negation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f446d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MunchWithAdd({'data': [['The falafel was delicious.', 'The falafel was not delicious.'], ['Everything was awful and there were flies in the soup!', 'Everything was not awful and there were flies in the soup!'], ['It was the best food me and Kathy in a long time.', 'It was not the best food me and Kathy in a long time.'], ['Jimmy hates lettuce...', \"Jimmy doesn't hate lettuce...\"], ['Restaurants in Germany are always great!', 'Restaurants in Germany are not always great!']]})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e1ad431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The falafel was delicious.', 'The falafel was not delicious.']\n",
      "['Everything was awful and there were flies in the soup!', 'Everything was not awful and there were flies in the soup!']\n",
      "['It was the best food me and Kathy in a long time.', 'It was not the best food me and Kathy in a long time.']\n",
      "['Jimmy hates lettuce...', \"Jimmy doesn't hate lettuce...\"]\n",
      "['Restaurants in Germany are always great!', 'Restaurants in Germany are not always great!']\n"
     ]
    }
   ],
   "source": [
    "for row in _data.data:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d88567e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 10 examples\n",
      "Test cases:      5\n",
      "Fails (rate):    3 (60.0%)\n",
      "\n",
      "Example fails:\n",
      "0.5 Jimmy hates lettuce...\n",
      "0.1 Jimmy doesn't hate lettuce...\n",
      "\n",
      "----\n",
      "1.0 Restaurants in Germany are always great!\n",
      "1.0 Restaurants in Germany are not always great!\n",
      "\n",
      "----\n",
      "0.7 It was the best food me and Kathy in a long time.\n",
      "0.7 It was not the best food me and Kathy in a long time.\n",
      "\n",
      "----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8259be1f7985449397129de26825f6af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TestSummarizer(stats={'npassed': 2, 'nfailed': 3, 'nfiltered': 0}, summarizer={'name': None, 'description': No…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = DIR(**_data, expect=expect_fn)\n",
    "test.run(wrapped_pp)\n",
    "\n",
    "test.summary()\n",
    "test.visual_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074ba7cc",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Prepare a list of five short movie reviews. Write one invariance test and one directionality expectation test. Check if the `TextBlob` sentiment classifier can pass your tests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
